{"cells":[{"cell_type":"markdown","metadata":{"id":"zje59_CLkH-h"},"source":["<b>Kagawa et al. Predicting elevated natriuretic peptide in chest radiography: Emerging utilization gap for artificial intelligence. European Heart Journal - Imaging Methods and Practice, qyae064, https://doi.org/10.1093/ehjimp/qyae064<br>\n","<br>\n","Python sample code: the final model (ensemble model), BNP cut-off 200 pg/mL<br></b>\n","<br>\n","Version 1.0.1 July 05, 2024: First release.<br>\n","<br>\n","The sample codes are utilized in Google Colab or Jupyter Notebook.<br>\n","<br>\n","In Google Colab or Jupyter Notebook, either click the arrow on the cell or select the cell and use the appropriate shortcut to execute it (Shift + Enter).<br>"]},{"cell_type":"markdown","metadata":{"id":"IqfhlrlbkH-j"},"source":["1. If you utilize in Google Colab, mount Google Colab.<br>\n","2. Install tensorflow_addons."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14415,"status":"ok","timestamp":1720061303446,"user":{"displayName":"Eisuke Kagawa","userId":"09263632618654374896"},"user_tz":-540},"id":"AfbFLdD_kH-j","outputId":"b5e45dd4-4824-4693-c228-e4789189a18b"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install tensorflow_addons\n","!pip install vit_keras"]},{"cell_type":"markdown","metadata":{"id":"p_nK8IMhkH-k"},"source":["2. Import libraries.<br>\n","Please ignore TFA warning.<br>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7739,"status":"ok","timestamp":1720061313965,"user":{"displayName":"Eisuke Kagawa","userId":"09263632618654374896"},"user_tz":-540},"id":"8j9VbP_rkH-k","outputId":"0ce8f16e-2ab8-49d1-b0f6-d777a7c77fc7"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import os\n","from vit_keras import vit                                    \n","from vit_keras.vit import preprocess_inputs"]},{"cell_type":"markdown","metadata":{"id":"Ez6GS_NPkH-k"},"source":["3. Move or copy the chest radiography image you want to test by specifying the 'image_file_path'.<br>\n","Example:<br>\n","image_file_path = '/content/drive/MyDrive/image_file.png'<br>"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["image_file_path = '/content/drive/MyDrive/image_file.png'"]},{"cell_type":"markdown","metadata":{},"source":["4. Visualize the image.<br>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BACKGROUND_COLOR = (0, 0, 0) #black\n","\n","def load_and_preprocess_image(path, target_size):   \n","    try:\n","        im = Image.open(path).convert(\"RGB\")\n","        width, height = im.size\n","        if width > height:\n","            result = Image.new(im.mode, (width, width), BACKGROUND_COLOR)\n","            result.paste(im, (0, (width - height) // 2))\n","        elif width < height:\n","            result = Image.new(im.mode, (height, height), BACKGROUND_COLOR)\n","            result.paste(im, ((height - width) // 2, 0))\n","        else:\n","            result = im\n","    except OSError as e:\n","        pass\n","    result = result.resize((target_size, target_size))\n","    image_array = img_to_array(result)\n","    return image_array\n","\n","print('File: ', image_file_path)\n","image_array = load_and_preprocess_image(image_file_path, 224)\n","plt. imshow(image_array.astype('uint8'))\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["5. Save the 31 models with the BNP cut-off 200 pg/mL under the specified 'model_dir_path'.  \n","Example:  \n","model_dir_path = '/content/drive/MyDrive/BNP_CO_200'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","model_dir_path = '/content/drive/MyDrive/BNP_CO_200'\n","\n","MODEL_PATH = []\n","TARGET_SIZE_ARR = []\n","MODEL_PREPROCESS_INPUT = []\n","\n","def preprocess_as_itis(x):\n","    return x\n","\n","def preprocess_input_mlpmixer(x):\n","    return x/255\n","\n","# VGG16 68 ep acc 0.842 pre 0.868 rec 0.805 spec 0.879 f1 0.835 ROC 0.919 PR 0.926\n","MODEL_NUMBER = 0\n","MODEL_PATH.append(model_dir_path + '/model_0.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.vgg16.preprocess_input)\n","\n","# VGG19 170 ep acc 0.838 pre 0.869 rec 0.793 spec 0.882 f1 0.829 ROC 0.913 PR 0.918\n","MODEL_NUMBER = 1\n","MODEL_PATH.append(model_dir_path + '/model_1.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.vgg19.preprocess_input)\n","\n","# InceptionResNetV2_3 45 ep acc 0.829 pre 0.852 rec 0.793 spec 0.864 f1 0.822 roc 0.909 pr 0.909\n","MODEL_NUMBER = 2\n","MODEL_PATH.append(model_dir_path + '/model_2.h5')\n","TARGET_SIZE_ARR.append(299)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.inception_resnet_v2.preprocess_input)\n","\n","# Xception relu 31 ep acc 0.791 PR *0.916, rec 0.638, spec *0.942, f1 0.752, ROC 0.899, PR 0.904\n","MODEL_NUMBER = 3\n","MODEL_PATH.append(model_dir_path + '/model_3.h5')\n","TARGET_SIZE_ARR.append(299)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.xception.preprocess_input)\n","\n","# Xception 45 ep acc 0.840 PR 0.862, rec 0.807, spec 0.873, f1 0.834, ROC *0.919, PR *0.926\n","MODEL_NUMBER = 4\n","MODEL_PATH.append(model_dir_path + '/model_4.h5')\n","TARGET_SIZE_ARR.append(299)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.xception.preprocess_input)\n","\n","MODEL_NUMBER = 5\n","# MobileNetV3Small 86 ep acc 0.809 pre 0.876 rec 0.717 sepc 0.900 f1 0.789 roc 0.914 pr 0.918\n","MODEL_PATH.append(model_dir_path + '/model_5.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","# MobileNetV3Large_3 76 ep acc 0.837 pre 0.863 rec 0.799 sepc 0.875 f1 0.830 roc 0.912 pr 0.917\n","MODEL_NUMBER = 6\n","MODEL_PATH.append(model_dir_path + '/model_6.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 7\n","# ResNet-RS101 78 ep acc 0.823 pre 0.884 rec 0.741 spec 0.904 f1 0.806 roc 0.902 pr 0.909\n","MODEL_PATH.append(model_dir_path + '/model_7.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 8\n","# ResNet-RS200 69 ep acc 0.823 pre 0.879 rec 0.745 spc 0.899 f1 0.807 roc 0.903 pr 0.906\n","MODEL_PATH.append(model_dir_path + '/model_8.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 9\n","# EfficientNetV2B0_2 60 ep acc 0.819 pr 0.841 rec 0.784 spec 0.854 f1 0.811 roc 0.900 pr 0.900\n","MODEL_PATH.append(model_dir_path + '/model_9.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 10\n","# EfficientNetV2B1_4 48 ep acc 0.831, pre 0.847, rec 0.806, spec 0.856, f1 0.826, roc 0.904, pr 0.909\n","MODEL_PATH.append(model_dir_path + '/model_10.h5')\n","TARGET_SIZE_ARR.append(240)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 11\n","# EfficientNetV2B2_3 43 ep acc 0.821 pre 0.801 rec 0.852 spec 0.791 f1 0.826 roc 0.907 pre 0.912\n","MODEL_PATH.append(model_dir_path + '/model_11.h5')\n","TARGET_SIZE_ARR.append(260)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 12\n","# EfficientNetV2B3_3 39 ep acc 0.825 pre 0.873 rec 0.758 spec 0.891 f1 0.811 roc 0.908 pr 0.908\n","MODEL_PATH.append(model_dir_path + '/model_12.h5')\n","TARGET_SIZE_ARR.append(300)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 13\n","# EfficientNetV2S_2 40 ep acc 0.844 pre 0.867 rec 0.810 spec 0.878 f1 0.837 roc 0.914 pre 0.919\n","MODEL_PATH.append(model_dir_path + '/model_13.h5')\n","TARGET_SIZE_ARR.append(384)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 14\n","# EfficientNetV2M_2 39 ep acc 0.835 pre 0.853 rec 0.807 spec 0.863 f1 0.830 roc 0.915 pr 0.919\n","MODEL_PATH.append(model_dir_path + '/model_14.h5')\n","TARGET_SIZE_ARR.append(480)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 15\n","# EfficientNetV2L 42 ep acc 0.832 pre 0.839 rec 0.819 spec 0.845 f1 b0.829 roc 0.909 pr 0.906\n","MODEL_PATH.append(model_dir_path + '/model_15.h5')\n","TARGET_SIZE_ARR.append(480)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 16\n","# ConvNeXtTiny 52 ep acc 0.831 pre 0.865 rec 0.781 spec 0.879 f1 0.821 roc 0.922 pr 0.924\n","MODEL_PATH.append(model_dir_path + '/model_16.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 17\n","# ConvNeXtSmall 46 ep acc 0.827 pre 0.866 rec 0.772 sepc 0.882 f1 0.816 roc 0.918 pr 0.922\n","MODEL_PATH.append(model_dir_path + '/model_17.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 18\n","# ConvNeXtBase 41 ep acc 0.830 pre 0.834 rec 0.820 spec 0.839 f1 0.827 roc 0.910 pre 0.913\n","MODEL_PATH.append(model_dir_path + '/model_18.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 19\n","# ConvNeXtLarge_3 36 ep acc 0.832 pre 0.835 rec 0.825 spec 0.839 f1 0.830 roc 0.906 pr 0.908\n","MODEL_PATH.append(model_dir_path + '/model_19.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 20\n","# ConvNeXtXLarge 37 ep acc 0.820 pre 0.889 rec 0.727 spec 0.911 f1 0.800 roc 0.913 pre 0.919\n","MODEL_PATH.append(model_dir_path + '/model_20.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_as_itis)\n","\n","MODEL_NUMBER = 21\n","# Vit-b16 384 116 ep acc 0.828 pre 0.903 rec 0.733 spec 0.922 f1 0.809 oroc 0.919 pre 0.924\n","MODEL_PATH.append(model_dir_path + '/model_21.h5')\n","TARGET_SIZE_ARR.append(384)\n","MODEL_PREPROCESS_INPUT.append(preprocess_inputs)\n","\n","MODEL_NUMBER = 22\n","# Vit-b16 320 43 ep acc 0.811 pre 0.863 rec 0.7365 pspec 0.885 f1 0.794 roc 0.905 pr 0.909\n","MODEL_PATH.append(model_dir_path + '/model_22.h5')\n","TARGET_SIZE_ARR.append(320)\n","MODEL_PREPROCESS_INPUT.append(preprocess_inputs)\n","\n","MODEL_NUMBER = 23\n","# Vit-b16 224 150 ep acc 0.827 pre 0.816 rec 0.843 sepc 0.812 f1 0.829 roc 0.906 pre 0.912\n","MODEL_PATH.append(model_dir_path + '/model_23.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_inputs)\n","\n","MODEL_NUMBER = 24\n","# MLPMixerB32 66 ep acc 0.817 pre 0.856 rec 0.759 spec 0.874 f1 0.804 roc 0.901 pre 0.898\n","MODEL_PATH.append(model_dir_path + '/model_24.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_input_mlpmixer)\n","\n","MODEL_NUMBER = 25\n","# MLPMixerB32_3 200 ep acc 0.768 pre 0.716 rec 0.884 spec 0.653 f1 0.791 roc 0.864 pr 0.862\n","MODEL_PATH.append(model_dir_path + '/model_25.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_input_mlpmixer)\n","\n","MODEL_NUMBER = 26\n","# VGG16_2 384*384*3 210 ep acc 0.823 pre 0.877 rec 0.749 apec 0.897 f1 0.808 roc 0.907 pr 0.914\n","MODEL_PATH.append(model_dir_path + '/model_26.h5')\n","TARGET_SIZE_ARR.append(384)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.vgg16.preprocess_input)\n","\n","MODEL_NUMBER = 27\n","# MLPMixerB32_3 150 ep acc 0.605 pre 0.558 rec 0.985 spec 0.231 roc 0.834 pr 0.817\n","MODEL_PATH.append(model_dir_path + '/model_27.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_input_mlpmixer)\n","\n","MODEL_NUMBER = 28\n","# Vit-b16 224 100 ep acc 0.822 pre 0.816 rec 0.828 spec 0.816 f1 0.822 roc 0.906 prr 0.909\n","MODEL_PATH.append(model_dir_path + '/model_28.h5')\n","TARGET_SIZE_ARR.append(224)\n","MODEL_PREPROCESS_INPUT.append(preprocess_inputs)\n","\n","MODEL_NUMBER = 29\n","# VGG19_2 384 53 ep acc 0.841 pre 0.871 rec 0.799 spec 0.883 f1 0.833 roc 0.915 pr 0.923\n","MODEL_PATH.append(model_dir_path + '/model_29.h5')\n","TARGET_SIZE_ARR.append(384)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.vgg19.preprocess_input)\n","\n","MODEL_NUMBER = 30\n","# VGG19_2 384 100 ep acc 0.834 pre 0.844 rec 0.816 spec 0.852 f1 0.830 roc 0.910 pr 0.917\n","MODEL_PATH.append(model_dir_path + '/model_30.h5')\n","TARGET_SIZE_ARR.append(384)\n","MODEL_PREPROCESS_INPUT.append(tf.keras.applications.vgg19.preprocess_input)\n","\n","MODEL_N = len(MODEL_PATH)\n","MODEL_MAJORITY_N = (MODEL_N + 1) // 2\n","print('Model N = ', MODEL_N)\n","\n","for i in range(MODEL_N):\n","    if os.path.exists(MODEL_PATH[i]) == False:\n","        print(f'There is no directory {MODEL_PATH[i]}.')"]},{"cell_type":"markdown","metadata":{},"source":["6. Ensemble prediction (31 models).<br>\n","The ensemble results will be output sequentially up to the model that has been executed.<br>\n","Execution may be interrupted if there is insufficient memory or other issues.<br>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","probs, preds = np.zeros(MODEL_N), np.zeros(MODEL_N)\n","preds_pos_num = 0\n","custom_objects = {\"Addons>mish\": tfa.activations.mish}\n","for i in range(MODEL_N):\n","    print('Model No.', i)\n","\n","    image_array = load_and_preprocess_image(image_file_path, TARGET_SIZE_ARR[i])\n","    print('Shape of image: ', image_array.shape)\n","\n","    model = tf.keras.models.load_model(MODEL_PATH[i], custom_objects=custom_objects)\n","    img_array = np.expand_dims(image_array, axis=0)\n","    preprocess_input = MODEL_PREPROCESS_INPUT[i]\n","    tmp_probs = model.predict(preprocess_input(img_array))\n","    probs[i] = tmp_probs[0][1]\n","    if probs[i] < 0.5:\n","        preds[i] = 0\n","    else:\n","        preds[i] = 1\n","    print('Single model: ', MODEL_PATH[i])\n","    print('The provability of BNP >= 200 pg/mL (single model):', probs[i])\n","    print('Soft ensemble provability: ', np.sum(probs) / float(i + 1))\n","    print('Hard ensemble: ', int(np.sum(preds)), '/', i + 1, '=', np.sum(preds) / float(i+1), '\\n')\n","\n","print('Calculated 31 models.')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1x3BO0EYcZZYuMBDOOyXvSsDVtnvO9WWV","timestamp":1647529276850}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":0}
